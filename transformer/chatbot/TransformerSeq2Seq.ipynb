{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TransformerSeq2Seq.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j65i49dU8Cjs","executionInfo":{"status":"ok","timestamp":1638257606574,"user_tz":-540,"elapsed":20798,"user":{"displayName":"이규봉","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03178481304664747452"}},"outputId":"06702687-d047-4038-b480-89cb3979f6ab"},"source":["from google.colab import drive\n","drive.mount(\"/gdrive\", force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"0zVeC5Mf8C-S","executionInfo":{"status":"ok","timestamp":1638257612449,"user_tz":-540,"elapsed":5880,"user":{"displayName":"이규봉","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03178481304664747452"}}},"source":["from torch.utils.data import (DataLoader, TensorDataset)\n","from torch import nn\n","from tqdm import tqdm\n","import numpy as np\n","import torch\n","import os\n","\n","class TransformerChat(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","\n","        # 전체 단어(음절) 개수\n","        self.vocab_size = config[\"vocab_size\"]\n","\n","        # 단어(음절) 벡터 크기\n","        self.embedding_size = config['embedding_size']\n","\n","        # Transformer의 Attention Head 개수\n","        self.num_heads = config['num_heads']\n","\n","        # Transformer Encoder의 Layer 수\n","        self.num_encoder_layers = config['num_encoder_layers']\n","\n","        # Transformer Decoder의 Layer 수\n","        self.num_decoder_layers = config['num_decoder_layers']\n","\n","        # 입력 Sequence의 최대 길이\n","        self.max_length = config['max_length']\n","\n","        # Transformer 내부 FNN 크기\n","        self.hidden_size = config['hidden_size']\n","\n","        # Token Embedding Matrix 선언\n","        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_size)\n","\n","        # Transformer Encoder-Decoder 설계(선언)\n","        self.transformer = nn.Transformer(d_model=self.embedding_size, nhead=self.num_heads, num_encoder_layers=self.num_encoder_layers,\n","                                          num_decoder_layers=self.num_decoder_layers, dim_feedforward=self.hidden_size)\n","       \n","        # 입력 길이 L에 대한 (L X L) mask 생성: 이전 토큰들의 정보만을 반영하기 위한 mask\n","        #       [[1, -inf, -inf, -inf],\n","        #        [1,    1, -inf, -inf],\n","        #               ......\n","        #        [1,    1,    1,    1]]\n","        # 이곳을 채우세요.\n","        self.mask = self.transformer.generate_square_subsequent_mask(self.max_length).cuda()\n","\n","        # 전체 단어 분포로 변환하기 위한 linear\n","        # 이곳을 채우세요.\n","        self.projection_layer = nn.Linear(self.embedding_size, self.vocab_size)\n","\n","    def forward(self, enc_inputs, dec_inputs):\n","\n","        # enc_inputs: [batch, seq_len], dec_inputs: [batch, seq_len]\n","        # enc_input_features: [batch, seq_len, emb_size] -> [seq_len, batch, emb_size]\n","        # 이곳을 채우세요.\n","        enc_input_features = self.embeddings(enc_inputs).transpose(0, 1)\n","\n","        # dec_input_features: [batch, seq_len, emb_size] -> [seq_len, batch, emb_size]\n","        # 이곳을 채우세요.\n","        dec_input_features = self.embeddings(dec_inputs).transpose(0, 1)\n","\n","        # dec_output_features: [seq_len, batch, emb_size]\n","        dec_output_features = self.transformer(src=enc_input_features, tgt=dec_input_features, src_mask = self.mask, tgt_mask = self.mask)\n","\n","        # hypothesis : [seq_len, batch, vocab_size]\n","        hypothesis = self.projection_layer(dec_output_features)\n","\n","        return hypothesis"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmYHzEcU8nRp","executionInfo":{"status":"ok","timestamp":1638257612450,"user_tz":-540,"elapsed":8,"user":{"displayName":"이규봉","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03178481304664747452"}}},"source":["# 어휘사전(vocabulary) 생성 함수\n","def load_vocab(file_dir):\n","\n","    with open(file_dir,'r',encoding='utf8') as vocab_file:\n","        char2idx = {}\n","        idx2char = {}\n","        index = 0\n","        for char in vocab_file:\n","            char = char.strip()\n","            char2idx[char] = index\n","            idx2char[index] = char\n","            index+=1\n","\n","    return char2idx, idx2char\n","\n","# 문자 입력열을 인덱스로 변환하는 함수\n","def convert_data2feature(config, input_sequence, char2idx, decoder_input=False):\n","\n","    # 고정 길이 벡터 생성\n","    input_features = np.zeros(config[\"max_length\"], dtype=np.int)\n","\n","    if decoder_input:\n","        # Decoder Input은 Target Sequence에서 Right Shift\n","        # Target Sequence :         [\"안\",\"녕\",\"하\",\"세\",\"요\", \"</S>\" ]\n","        # Decoder Input Sequence :  [\"<S>\", \"안\",\"녕\",\"하\",\"세\",\"요\"]\n","        # 이곳을 채우세요.\n","        input_sequence = \" \".join([\"<S>\"] + input_sequence.split()[:-1])\n","\n","    for idx,token in enumerate(input_sequence.split()):\n","        if token in char2idx.keys():\n","            input_features[idx] = char2idx[token]\n","        else:\n","            input_features[idx] = char2idx['<UNK>']\n","\n","    return input_features\n","\n","# 데이터 읽기 함수\n","def load_dataset(config):\n","\n","    # 어휘사전 읽어오기\n","    char2idx, idx2char = load_vocab(config['vocab_file'])\n","\n","    file_dir = config['train_file']\n","    data_file = open(file_dir,'r',encoding='utf8').readlines()\n","\n","    # 데이터를 저장하기 위한 리스트 생성\n","    enc_inputs, dec_inputs, dec_outputs = [], [], []\n","\n","    for line in tqdm(data_file):\n","\n","        line = line.strip().split('\\t')\n","\n","        input_sequence = line[0]\n","        output_sequence = line[1]\n","\n","        enc_inputs.append(convert_data2feature(config, input_sequence, char2idx))\n","        dec_inputs.append(convert_data2feature(config, output_sequence, char2idx, True))\n","        dec_outputs.append(convert_data2feature(config, output_sequence, char2idx))\n","\n","    # 전체 데이터를 저장하고 있는 리스트를 텐서 형태로 변환\n","    enc_inputs = torch.tensor(enc_inputs, dtype=torch.long)\n","    dec_inputs = torch.tensor(dec_inputs, dtype=torch.long)\n","    dec_outputs = torch.tensor(dec_outputs, dtype=torch.long)\n","\n","    return enc_inputs, dec_inputs, dec_outputs, char2idx, idx2char"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"fJ-bIKnw-1D8","executionInfo":{"status":"ok","timestamp":1638257612450,"user_tz":-540,"elapsed":6,"user":{"displayName":"이규봉","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03178481304664747452"}}},"source":["# 텐서를 리스트로 변환하는 함수\n","def tensor2list(input_tensor):\n","    return input_tensor.cpu().detach().numpy().tolist()\n","\n","def do_test(config, model, word2idx, idx2word, input_sequence=\"오늘 약속있으세요?\"):\n","\n","    # 평가 모드 셋팅\n","    model.eval()\n","\n","    # 입력된 문자열의 음절을 공백 단위 토큰으로 변환. 공백은 <SP>로 변환: \"오늘 약속\" -> \"오 늘 <SP> 약 속\"\n","    input_sequence = \" \".join([e if e != \" \" else \"<SP>\" for e in input_sequence])\n","\n","    # 텐서 변환: [1, seq_len]\n","    enc_inputs = torch.tensor([convert_data2feature(config, input_sequence, word2idx)], dtype=torch.long).cuda()\n","    \n","    # input_ids : [1, seq_len] -> 첫번째 디코더 입력 \"<S>\" 만들기\n","    dec_inputs = torch.tensor([convert_data2feature(config, \"\", word2idx, True)], dtype=torch.long).cuda()\n","    \n","    # 시스템 응답 문자열 초기화\n","    response = ''\n","\n","    # 최대 입력 길이 만큼 Decoding Loop\n","    for decoding_step in range(config['max_length']-1):\n","\n","        # dec_outputs: [vocab_size]\n","        dec_outputs = model(enc_inputs, dec_inputs)[decoding_step, 0, :]\n","        # 가장 큰 출력을 갖는 인덱스 얻어오기\n","        dec_output_idx = np.argmax(tensor2list(dec_outputs))\n","\n","        # 생성된 토큰은 dec_inputs에 추가 (첫번째 차원은 배치)\n","        dec_inputs[0][decoding_step+1] = dec_output_idx\n","\n","        # </S> 심볼 생성 시, Decoding 종료\n","        if idx2word[dec_output_idx] == \"</S>\":\n","            break\n","\n","        # 생성 토큰 추가\n","        response += idx2word[dec_output_idx]\n","    \n","    # <SP>를 공백으로 변환한 후 응답 문자열 출력\n","    print(response.replace(\"<SP>\", \" \"))\n","\n","def test(config):\n","\n","    # 어휘사전 읽어오기\n","    word2idx, idx2word = load_vocab(config['vocab_file'])\n","\n","    # Transformer Seq2Seq 모델 객체 생성\n","    model = TransformerChat(config).cuda()\n","\n","    # 학습한 모델 파일로부터 가중치 불러옴\n","    model.load_state_dict(torch.load(os.path.join(config[\"output_dir\"], config[\"trained_model_name\"])))\n","\n","    while(True):\n","        input_sequence = input(\"문장을 입력하세요. (종료는 exit을 입력하세요.) : \")\n","        if input_sequence == 'exit':\n","            break\n","        do_test(config, model, word2idx, idx2word, input_sequence)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ow01KJjz-416","executionInfo":{"status":"ok","timestamp":1638257612451,"user_tz":-540,"elapsed":6,"user":{"displayName":"이규봉","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03178481304664747452"}}},"source":["def train(config):\n","\n","    # Transformer Seq2Seq 모델 객체 생성\n","    model = TransformerChat(config).cuda()\n","\n","    # 데이터 읽기\n","    enc_inputs, dec_inputs, dec_outputs, word2idx, idx2word = load_dataset(config)\n","\n","    # TensorDataset/DataLoader를 통해 배치(batch) 단위로 데이터를 나누고 셔플(shuffle)\n","    train_features = TensorDataset(enc_inputs, dec_inputs, dec_outputs)\n","    train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n","\n","    # 크로스엔트로피 손실 함수\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    # 옵티마이저 함수 지정\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learn_rate\"])\n","\n","    for epoch in range(config[\"epoch\"] + 1):\n","\n","        for (step, batch) in enumerate(train_dataloader):\n","\n","            # 학습 모드 셋팅\n","            model.train()\n","          \n","            # batch = (enc_inputs[step], dec_inputs[step], dec_outputs)*batch_size\n","            # .cuda()를 통해 메모리에 업로드\n","            batch = tuple(t.cuda() for t in batch)\n","\n","            # 역전파 변화도 초기화\n","            optimizer.zero_grad()\n","\n","            enc_inputs, dec_inputs, dec_outputs = batch\n","\n","            # hypothesis: [seq_len, batch, vocab_size] -> [seq_len*batch, vocab_size]\n","            # 이곳을 채우세요.\n","            hypothesis = model(enc_inputs, dec_inputs).view(-1, config[\"vocab_size\"])\n","\n","            # labels: [batch, seq_len] -> [seq_len, batch] -> [seq_len(max_length)*batch]\n","            labels = dec_outputs.transpose(0, 1)\n","            labels = labels.reshape(config[\"max_length\"]*dec_inputs.size(0))\n","\n","            # 비용 계산 및 역전파 수행: cross_entopy 내부에서 labels를 원핫벡터로 변환 (골드레이블은 항상 1차원으로 입력)\n","            loss = loss_func(hypothesis, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # 200 배치마다 중간 결과 출력\n","            if (step+1)% 200 == 0:\n","                print(\"Current Step : {0:d} / {1:d}\\tCurrent Loss : {2:f}\".format(step+1, int(len(enc_inputs) / config['batch_size']), loss.item()))\n","                # 생성 문장을 확인하기 위한 함수 호출\n","                # do_test(config, model, word2idx, idx2word)\n","\n","        # 에폭마다 가중치 저장\n","        torch.save(model.state_dict(), os.path.join(config[\"output_dir\"], \"epoch_{0:d}.pt\".format(epoch)))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fk1BLZ-VrHe","executionInfo":{"status":"ok","timestamp":1638266177966,"user_tz":-540,"elapsed":8565521,"user":{"displayName":"이규봉","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03178481304664747452"}},"outputId":"48e79e75-3a5e-405d-97d5-5da03935dde0"},"source":["if(__name__==\"__main__\"):\n","\n","    root_dir = \"/gdrive/My Drive/colab/transformer/chatbot/\"\n","    output_dir = os.path.join(root_dir, \"output\")\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","    config = {\"mode\": \"train\",\n","              \"vocab_file\": os.path.join(root_dir, \"vocab.txt\"),\n","              \"train_file\": os.path.join(root_dir, \"train.txt\"),\n","              \"trained_model_name\":\"epoch_{}.pt\".format(10),\n","              \"output_dir\":output_dir,\n","              \"epoch\": 10,\n","              \"learn_rate\":0.00005,\n","              \"num_encoder_layers\": 6,\n","              \"num_decoder_layers\": 6,\n","              \"num_heads\": 4,\n","              \"max_length\": 20,\n","              \"batch_size\": 128,\n","              \"embedding_size\": 256,\n","              \"hidden_size\": 512,\n","              \"vocab_size\": 4427\n","            }\n","\n","    if(config[\"mode\"] == \"train\"):\n","        train(config)\n","    else:\n","        test(config)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 547958/547958 [00:11<00:00, 47631.54it/s]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"]},{"output_type":"stream","name":"stdout","text":["Current Step : 200 / 1\tCurrent Loss : 2.770959\n","Current Step : 400 / 1\tCurrent Loss : 2.301779\n","Current Step : 600 / 1\tCurrent Loss : 2.219465\n","Current Step : 800 / 1\tCurrent Loss : 2.165344\n","Current Step : 1000 / 1\tCurrent Loss : 2.164491\n","Current Step : 1200 / 1\tCurrent Loss : 2.045499\n","Current Step : 1400 / 1\tCurrent Loss : 2.117066\n","Current Step : 1600 / 1\tCurrent Loss : 1.969795\n","Current Step : 1800 / 1\tCurrent Loss : 1.763301\n","Current Step : 2000 / 1\tCurrent Loss : 1.869776\n","Current Step : 2200 / 1\tCurrent Loss : 1.985065\n","Current Step : 2400 / 1\tCurrent Loss : 1.954931\n","Current Step : 2600 / 1\tCurrent Loss : 1.900710\n","Current Step : 2800 / 1\tCurrent Loss : 1.893243\n","Current Step : 3000 / 1\tCurrent Loss : 1.877440\n","Current Step : 3200 / 1\tCurrent Loss : 1.785317\n","Current Step : 3400 / 1\tCurrent Loss : 1.736850\n","Current Step : 3600 / 1\tCurrent Loss : 1.780643\n","Current Step : 3800 / 1\tCurrent Loss : 1.697308\n","Current Step : 4000 / 1\tCurrent Loss : 2.010406\n","Current Step : 4200 / 1\tCurrent Loss : 1.824055\n","Current Step : 200 / 1\tCurrent Loss : 1.728073\n","Current Step : 400 / 1\tCurrent Loss : 1.618459\n","Current Step : 600 / 1\tCurrent Loss : 1.821693\n","Current Step : 800 / 1\tCurrent Loss : 1.701892\n","Current Step : 1000 / 1\tCurrent Loss : 1.688192\n","Current Step : 1200 / 1\tCurrent Loss : 1.712981\n","Current Step : 1400 / 1\tCurrent Loss : 1.675214\n","Current Step : 1600 / 1\tCurrent Loss : 1.577895\n","Current Step : 1800 / 1\tCurrent Loss : 1.776059\n","Current Step : 2000 / 1\tCurrent Loss : 1.659547\n","Current Step : 2200 / 1\tCurrent Loss : 1.665216\n","Current Step : 2400 / 1\tCurrent Loss : 1.682341\n","Current Step : 2600 / 1\tCurrent Loss : 1.907287\n","Current Step : 2800 / 1\tCurrent Loss : 1.676776\n","Current Step : 3000 / 1\tCurrent Loss : 1.664589\n","Current Step : 3200 / 1\tCurrent Loss : 1.607364\n","Current Step : 3400 / 1\tCurrent Loss : 1.690746\n","Current Step : 3600 / 1\tCurrent Loss : 1.660674\n","Current Step : 3800 / 1\tCurrent Loss : 1.770115\n","Current Step : 4000 / 1\tCurrent Loss : 1.666369\n","Current Step : 4200 / 1\tCurrent Loss : 1.556688\n","Current Step : 200 / 1\tCurrent Loss : 1.667217\n","Current Step : 400 / 1\tCurrent Loss : 1.696239\n","Current Step : 600 / 1\tCurrent Loss : 1.612458\n","Current Step : 800 / 1\tCurrent Loss : 1.649158\n","Current Step : 1000 / 1\tCurrent Loss : 1.664822\n","Current Step : 1200 / 1\tCurrent Loss : 1.655531\n","Current Step : 1400 / 1\tCurrent Loss : 1.750928\n","Current Step : 1600 / 1\tCurrent Loss : 1.631256\n","Current Step : 1800 / 1\tCurrent Loss : 1.698990\n","Current Step : 2000 / 1\tCurrent Loss : 1.586232\n","Current Step : 2200 / 1\tCurrent Loss : 1.748278\n","Current Step : 2400 / 1\tCurrent Loss : 1.578217\n","Current Step : 2600 / 1\tCurrent Loss : 1.706511\n","Current Step : 2800 / 1\tCurrent Loss : 1.576025\n","Current Step : 3000 / 1\tCurrent Loss : 1.762034\n","Current Step : 3200 / 1\tCurrent Loss : 1.585797\n","Current Step : 3400 / 1\tCurrent Loss : 1.797556\n","Current Step : 3600 / 1\tCurrent Loss : 1.528926\n","Current Step : 3800 / 1\tCurrent Loss : 1.656534\n","Current Step : 4000 / 1\tCurrent Loss : 1.601897\n","Current Step : 4200 / 1\tCurrent Loss : 1.588976\n","Current Step : 200 / 1\tCurrent Loss : 1.516731\n","Current Step : 400 / 1\tCurrent Loss : 1.476775\n","Current Step : 600 / 1\tCurrent Loss : 1.762167\n","Current Step : 800 / 1\tCurrent Loss : 1.612574\n","Current Step : 1000 / 1\tCurrent Loss : 1.673723\n","Current Step : 1200 / 1\tCurrent Loss : 1.636743\n","Current Step : 1400 / 1\tCurrent Loss : 1.607363\n","Current Step : 1600 / 1\tCurrent Loss : 1.573795\n","Current Step : 1800 / 1\tCurrent Loss : 1.586997\n","Current Step : 2000 / 1\tCurrent Loss : 1.537829\n","Current Step : 2200 / 1\tCurrent Loss : 1.620664\n","Current Step : 2400 / 1\tCurrent Loss : 1.621117\n","Current Step : 2600 / 1\tCurrent Loss : 1.504421\n","Current Step : 2800 / 1\tCurrent Loss : 1.697308\n","Current Step : 3000 / 1\tCurrent Loss : 1.560048\n","Current Step : 3200 / 1\tCurrent Loss : 1.635949\n","Current Step : 3400 / 1\tCurrent Loss : 1.558695\n","Current Step : 3600 / 1\tCurrent Loss : 1.642378\n","Current Step : 3800 / 1\tCurrent Loss : 1.539082\n","Current Step : 4000 / 1\tCurrent Loss : 1.543392\n","Current Step : 4200 / 1\tCurrent Loss : 1.539285\n","Current Step : 200 / 1\tCurrent Loss : 1.626667\n","Current Step : 400 / 1\tCurrent Loss : 1.501832\n","Current Step : 600 / 1\tCurrent Loss : 1.651313\n","Current Step : 800 / 1\tCurrent Loss : 1.626037\n","Current Step : 1000 / 1\tCurrent Loss : 1.571422\n","Current Step : 1200 / 1\tCurrent Loss : 1.672178\n","Current Step : 1400 / 1\tCurrent Loss : 1.548104\n","Current Step : 1600 / 1\tCurrent Loss : 1.453572\n","Current Step : 1800 / 1\tCurrent Loss : 1.543066\n","Current Step : 2000 / 1\tCurrent Loss : 1.634283\n","Current Step : 2200 / 1\tCurrent Loss : 1.393268\n","Current Step : 2400 / 1\tCurrent Loss : 1.501625\n","Current Step : 2600 / 1\tCurrent Loss : 1.503296\n","Current Step : 2800 / 1\tCurrent Loss : 1.532969\n","Current Step : 3000 / 1\tCurrent Loss : 1.572015\n","Current Step : 3200 / 1\tCurrent Loss : 1.608500\n","Current Step : 3400 / 1\tCurrent Loss : 1.615495\n","Current Step : 3600 / 1\tCurrent Loss : 1.518394\n","Current Step : 3800 / 1\tCurrent Loss : 1.469119\n","Current Step : 4000 / 1\tCurrent Loss : 1.628061\n","Current Step : 4200 / 1\tCurrent Loss : 1.694601\n","Current Step : 200 / 1\tCurrent Loss : 1.512493\n","Current Step : 400 / 1\tCurrent Loss : 1.455443\n","Current Step : 600 / 1\tCurrent Loss : 1.532245\n","Current Step : 800 / 1\tCurrent Loss : 1.475720\n","Current Step : 1000 / 1\tCurrent Loss : 1.513403\n","Current Step : 1200 / 1\tCurrent Loss : 1.488056\n","Current Step : 1400 / 1\tCurrent Loss : 1.552495\n","Current Step : 1600 / 1\tCurrent Loss : 1.574468\n","Current Step : 1800 / 1\tCurrent Loss : 1.614589\n","Current Step : 2000 / 1\tCurrent Loss : 1.602608\n","Current Step : 2200 / 1\tCurrent Loss : 1.556609\n","Current Step : 2400 / 1\tCurrent Loss : 1.486993\n","Current Step : 2600 / 1\tCurrent Loss : 1.550874\n","Current Step : 2800 / 1\tCurrent Loss : 1.514227\n","Current Step : 3000 / 1\tCurrent Loss : 1.509377\n","Current Step : 3200 / 1\tCurrent Loss : 1.503272\n","Current Step : 3400 / 1\tCurrent Loss : 1.683107\n","Current Step : 3600 / 1\tCurrent Loss : 1.492141\n","Current Step : 3800 / 1\tCurrent Loss : 1.515416\n","Current Step : 4000 / 1\tCurrent Loss : 1.544519\n","Current Step : 4200 / 1\tCurrent Loss : 1.634927\n","Current Step : 200 / 1\tCurrent Loss : 1.581537\n","Current Step : 400 / 1\tCurrent Loss : 1.397477\n","Current Step : 600 / 1\tCurrent Loss : 1.523305\n","Current Step : 800 / 1\tCurrent Loss : 1.458183\n","Current Step : 1000 / 1\tCurrent Loss : 1.671461\n","Current Step : 1200 / 1\tCurrent Loss : 1.649125\n","Current Step : 1400 / 1\tCurrent Loss : 1.497859\n","Current Step : 1600 / 1\tCurrent Loss : 1.495430\n","Current Step : 1800 / 1\tCurrent Loss : 1.512971\n","Current Step : 2000 / 1\tCurrent Loss : 1.383879\n","Current Step : 2200 / 1\tCurrent Loss : 1.657005\n","Current Step : 2400 / 1\tCurrent Loss : 1.643938\n","Current Step : 2600 / 1\tCurrent Loss : 1.390508\n","Current Step : 2800 / 1\tCurrent Loss : 1.557039\n","Current Step : 3000 / 1\tCurrent Loss : 1.543661\n","Current Step : 3200 / 1\tCurrent Loss : 1.431502\n","Current Step : 3400 / 1\tCurrent Loss : 1.695876\n","Current Step : 3600 / 1\tCurrent Loss : 1.430831\n","Current Step : 3800 / 1\tCurrent Loss : 1.585719\n","Current Step : 4000 / 1\tCurrent Loss : 1.535396\n","Current Step : 4200 / 1\tCurrent Loss : 1.503338\n","Current Step : 200 / 1\tCurrent Loss : 1.451311\n","Current Step : 400 / 1\tCurrent Loss : 1.503027\n","Current Step : 600 / 1\tCurrent Loss : 1.455559\n","Current Step : 800 / 1\tCurrent Loss : 1.544638\n","Current Step : 1000 / 1\tCurrent Loss : 1.692707\n","Current Step : 1200 / 1\tCurrent Loss : 1.594654\n","Current Step : 1400 / 1\tCurrent Loss : 1.532553\n","Current Step : 1600 / 1\tCurrent Loss : 1.436038\n","Current Step : 1800 / 1\tCurrent Loss : 1.550964\n","Current Step : 2000 / 1\tCurrent Loss : 1.476211\n","Current Step : 2200 / 1\tCurrent Loss : 1.516415\n","Current Step : 2400 / 1\tCurrent Loss : 1.557202\n","Current Step : 2600 / 1\tCurrent Loss : 1.399360\n","Current Step : 2800 / 1\tCurrent Loss : 1.458818\n","Current Step : 3000 / 1\tCurrent Loss : 1.535638\n","Current Step : 3200 / 1\tCurrent Loss : 1.472016\n","Current Step : 3400 / 1\tCurrent Loss : 1.514900\n","Current Step : 3600 / 1\tCurrent Loss : 1.485117\n","Current Step : 3800 / 1\tCurrent Loss : 1.635557\n","Current Step : 4000 / 1\tCurrent Loss : 1.562989\n","Current Step : 4200 / 1\tCurrent Loss : 1.536914\n","Current Step : 200 / 1\tCurrent Loss : 1.371531\n","Current Step : 400 / 1\tCurrent Loss : 1.430202\n","Current Step : 600 / 1\tCurrent Loss : 1.635300\n","Current Step : 800 / 1\tCurrent Loss : 1.501472\n","Current Step : 1000 / 1\tCurrent Loss : 1.505254\n","Current Step : 1200 / 1\tCurrent Loss : 1.346741\n","Current Step : 1400 / 1\tCurrent Loss : 1.550525\n","Current Step : 1600 / 1\tCurrent Loss : 1.567789\n","Current Step : 1800 / 1\tCurrent Loss : 1.509079\n","Current Step : 2000 / 1\tCurrent Loss : 1.395169\n","Current Step : 2200 / 1\tCurrent Loss : 1.523057\n","Current Step : 2400 / 1\tCurrent Loss : 1.448730\n","Current Step : 2600 / 1\tCurrent Loss : 1.479852\n","Current Step : 2800 / 1\tCurrent Loss : 1.436382\n","Current Step : 3000 / 1\tCurrent Loss : 1.537158\n","Current Step : 3200 / 1\tCurrent Loss : 1.583126\n","Current Step : 3400 / 1\tCurrent Loss : 1.612281\n","Current Step : 3600 / 1\tCurrent Loss : 1.468124\n","Current Step : 3800 / 1\tCurrent Loss : 1.539139\n","Current Step : 4000 / 1\tCurrent Loss : 1.569446\n","Current Step : 4200 / 1\tCurrent Loss : 1.493581\n","Current Step : 200 / 1\tCurrent Loss : 1.500700\n","Current Step : 400 / 1\tCurrent Loss : 1.495469\n","Current Step : 600 / 1\tCurrent Loss : 1.440959\n","Current Step : 800 / 1\tCurrent Loss : 1.516883\n","Current Step : 1000 / 1\tCurrent Loss : 1.523848\n","Current Step : 1200 / 1\tCurrent Loss : 1.503889\n","Current Step : 1400 / 1\tCurrent Loss : 1.462616\n","Current Step : 1600 / 1\tCurrent Loss : 1.585486\n","Current Step : 1800 / 1\tCurrent Loss : 1.627727\n","Current Step : 2000 / 1\tCurrent Loss : 1.516864\n","Current Step : 2200 / 1\tCurrent Loss : 1.382859\n","Current Step : 2400 / 1\tCurrent Loss : 1.507352\n","Current Step : 2600 / 1\tCurrent Loss : 1.442272\n","Current Step : 2800 / 1\tCurrent Loss : 1.447203\n","Current Step : 3000 / 1\tCurrent Loss : 1.625795\n","Current Step : 3200 / 1\tCurrent Loss : 1.504291\n","Current Step : 3400 / 1\tCurrent Loss : 1.501372\n","Current Step : 3600 / 1\tCurrent Loss : 1.542170\n","Current Step : 3800 / 1\tCurrent Loss : 1.305705\n","Current Step : 4000 / 1\tCurrent Loss : 1.392635\n","Current Step : 4200 / 1\tCurrent Loss : 1.527372\n","Current Step : 200 / 1\tCurrent Loss : 1.457447\n","Current Step : 400 / 1\tCurrent Loss : 1.584542\n","Current Step : 600 / 1\tCurrent Loss : 1.532958\n","Current Step : 800 / 1\tCurrent Loss : 1.376246\n","Current Step : 1000 / 1\tCurrent Loss : 1.565504\n","Current Step : 1200 / 1\tCurrent Loss : 1.326942\n","Current Step : 1400 / 1\tCurrent Loss : 1.543146\n","Current Step : 1600 / 1\tCurrent Loss : 1.484696\n","Current Step : 1800 / 1\tCurrent Loss : 1.428721\n","Current Step : 2000 / 1\tCurrent Loss : 1.495120\n","Current Step : 2200 / 1\tCurrent Loss : 1.428602\n","Current Step : 2400 / 1\tCurrent Loss : 1.575512\n","Current Step : 2600 / 1\tCurrent Loss : 1.602851\n","Current Step : 2800 / 1\tCurrent Loss : 1.560439\n","Current Step : 3000 / 1\tCurrent Loss : 1.482455\n","Current Step : 3200 / 1\tCurrent Loss : 1.481090\n","Current Step : 3400 / 1\tCurrent Loss : 1.496848\n","Current Step : 3600 / 1\tCurrent Loss : 1.403068\n","Current Step : 3800 / 1\tCurrent Loss : 1.475984\n","Current Step : 4000 / 1\tCurrent Loss : 1.613075\n","Current Step : 4200 / 1\tCurrent Loss : 1.504931\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qG_2UcZuwsGi","executionInfo":{"status":"ok","timestamp":1638266308114,"user_tz":-540,"elapsed":50459,"user":{"displayName":"이규봉","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03178481304664747452"}},"outputId":"4db7259c-700f-466b-fa33-1c8d95ff4883"},"source":["if(__name__==\"__main__\"):\n","\n","    root_dir = \"/gdrive/My Drive/colab/transformer/chatbot/\"\n","    output_dir = os.path.join(root_dir, \"output\")\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","    config = {\"mode\": \"test\",\n","              \"vocab_file\": os.path.join(root_dir, \"vocab.txt\"),\n","              \"train_file\": os.path.join(root_dir, \"train.txt\"),\n","              \"trained_model_name\":\"epoch_{}.pt\".format(10),\n","              \"output_dir\":output_dir,\n","              \"epoch\": 10,\n","              \"learn_rate\":0.00005,\n","              \"num_encoder_layers\": 6,\n","              \"num_decoder_layers\": 6,\n","              \"num_heads\": 4,\n","              \"max_length\": 20,\n","              \"batch_size\": 128,\n","              \"embedding_size\": 256,\n","              \"hidden_size\": 512,\n","              \"vocab_size\": 4427\n","            }\n","\n","    if(config[\"mode\"] == \"train\"):\n","        train(config)\n","    else:\n","        test(config)"],"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":["문장을 입력하세요. (종료는 exit을 입력하세요.) : 안녕\n","안녕하세요\n","문장을 입력하세요. (종료는 exit을 입력하세요.) : 이름이 뭐야\n","ㅋㅋㅋㅋㅋㅋㅋ\n","문장을 입력하세요. (종료는 exit을 입력하세요.) : 뭐해\n","나 일하고 있어\n","문장을 입력하세요. (종료는 exit을 입력하세요.) : 무슨일해?\n","나 일하고 있어\n","문장을 입력하세요. (종료는 exit을 입력하세요.) : exit\n"]}]},{"cell_type":"code","metadata":{"id":"cE4G-FAmRz9p"},"source":[""],"execution_count":null,"outputs":[]}]}